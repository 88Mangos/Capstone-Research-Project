{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Ethics\n",
    "This code makes use of the mitdeeplearning package (Amini, 2024) for the data loading function. \n",
    "The DB-VAE for Models 4-6 is inspired by the Debiasing Computer Vision Lab notebook from 6.S191.\n",
    "\n",
    "### Copyright 2024 MIT 6.S191 Introduction to Deep Learning. All Rights Reserved. \n",
    " \n",
    "Licensed under the MIT License. You may not use this file except in compliance \n",
    "with the License. Use and/or modification of this code outside of 6.S191 must \n",
    "reference: \n",
    "\n",
    "Â© MIT 6.S191: Introduction to Deep Learning \n",
    "http://introtodeeplearning.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports & Comet Setup ###\n",
    "COMET_API_KEY = # Insert KEY here\n",
    "import comet_ml\n",
    "\n",
    "### Create a Comet experiment to track our training run ###\n",
    "def create_experiment(project_name, params):\n",
    "    # end any prior experiments\n",
    "    if 'experiment' in locals():\n",
    "        experiment.end()\n",
    "\n",
    "    # initiate the comet experiment for tracking\n",
    "    experiment = comet_ml.Experiment(\n",
    "    api_key=COMET_API_KEY,\n",
    "    project_name=project_name)\n",
    "    # log our hyperparameters, defined above, to the experiment\n",
    "    for param, value in params.items():\n",
    "        experiment.log_parameter(param, value)\n",
    "    experiment.flush()\n",
    "\n",
    "    return experiment\n",
    "\n",
    "import os\n",
    "CWD = os.getcwd()\n",
    "print(CWD)\n",
    "\n",
    "### Data Visualization ###\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['font.family'] = \"Times New Roman\"\n",
    "\n",
    "### Data Loading ###\n",
    "import h5py\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "### The Rest ###\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras import backend, layers, utils\n",
    "import functools\n",
    "\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
    "assert COMET_API_KEY != \"\", \"Please insert your Comet API Key\"\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create our Standard ResNet50V2 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard CNN ###\n",
    "\n",
    "# Helper Functions\n",
    "def resize_images(x):\n",
    "    return tf.image.resize(x, (64, 64))\n",
    "\n",
    "# CNN Function\n",
    "def make_standard_ResNet50_V2(n_outputs = 1):\n",
    "    \n",
    "    Resize = tf.keras.layers.Lambda(resize_images)\n",
    "    Flatten = tf.keras.layers.Flatten\n",
    "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
    "    ResNet50V2 = tf.keras.applications.ResNet50V2(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\", # Utilizing Transfer Learning, also maintains consistency\n",
    "        input_tensor=None,\n",
    "        input_shape=(64,64,3),\n",
    "        pooling=None,\n",
    "        classes=1000,\n",
    "        classifier_activation=\"softmax\",\n",
    "    )\n",
    "    ResNet50V2 = tf.keras.Model(inputs = ResNet50V2.layers[1].input, \n",
    "                                outputs = ResNet50V2.layers[-1].output)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(Resize)\n",
    "    model.add(ResNet50V2)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dense(n_outputs, activation=None))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create our DB-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Decoder Network ###\n",
    "latent_dim = 100 # number of latent variables\n",
    "n_filters = 12 \n",
    "def make_decoder_network():\n",
    "    \"\"\"\n",
    "    Layer Types, Functional Definition\n",
    "    \"\"\"\n",
    "    Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu')\n",
    "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
    "    Reshape = tf.keras.layers.Reshape \n",
    "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "    LeakyReLU = tf.keras.layers.LeakyReLU\n",
    "    # Decoder\n",
    "    decoder = tf.keras.Sequential([\n",
    "        Dense(units=4*4*6*n_filters),\n",
    "        Reshape(target_shape=(4,4,6*n_filters)),\n",
    "\n",
    "        Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "\n",
    "        Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "\n",
    "        Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "\n",
    "        Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DB_VAE Helper Functions ###\n",
    "\n",
    "### VAE Reparameterization ###\n",
    "def sampling(z_mean, z_logsigma):\n",
    "    batch, latent_dim = z_mean.shape\n",
    "    epsilon = tf.random.normal(shape=(batch, latent_dim))\n",
    "    z = z_mean + tf.math.exp(0.5 * z_logsigma) * epsilon\n",
    "    return z\n",
    "\n",
    "### Defining the VAE loss function ###\n",
    "def vae_loss_function(x, x_recon, mu, logsigma, kl_weight=0.0005):\n",
    "  latent_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1.0 - logsigma, axis=1)\n",
    "  reconstruction_loss = tf.reduce_mean(tf.abs(x-x_recon), axis=(1,2,3))\n",
    "  vae_loss = kl_weight * latent_loss + reconstruction_loss\n",
    "  return vae_loss\n",
    "\n",
    "### Loss function for DB-VAE ###\n",
    "def debiasing_loss_function(x, x_pred, y, y_logit, mu, logsigma):\n",
    "  vae_loss = vae_loss_function(x, x_pred, mu, logsigma)\n",
    "  classification_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_logit)\n",
    "  malignance_indicator = tf.cast(tf.equal(y, 1), tf.float32)\n",
    "  total_loss = tf.reduce_mean(\n",
    "      classification_loss +\n",
    "      malignance_indicator * vae_loss\n",
    "  )\n",
    "  return total_loss, classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining and creating the DB-VAE ###\n",
    "\n",
    "class DB_VAE(tf.keras.Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(DB_VAE, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    # Define the number of outputs for the encoder. Recall that we have\n",
    "    # `latent_dim` latent variables, as well as a supervised output for the\n",
    "    # classification.\n",
    "    num_encoder_dims = 2*self.latent_dim + 1\n",
    "\n",
    "    self.encoder = make_standard_ResNet50_V2(num_encoder_dims)\n",
    "    self.decoder = make_decoder_network()\n",
    "\n",
    "  def encode(self, x):\n",
    "    encoder_output = self.encoder(x)\n",
    "    y_logit = tf.expand_dims(encoder_output[:, 0], -1)\n",
    "    z_mean = encoder_output[:, 1:self.latent_dim+1]\n",
    "    z_logsigma = encoder_output[:, self.latent_dim+1:]\n",
    "\n",
    "    return y_logit, z_mean, z_logsigma\n",
    "\n",
    "  def reparameterize(self, z_mean, z_logsigma):\n",
    "    z = sampling(z_mean, z_logsigma)\n",
    "    return z\n",
    "\n",
    "  def decode(self, z):\n",
    "    reconstruction = self.decoder(z)\n",
    "    return reconstruction\n",
    "\n",
    "  def call(self, x):\n",
    "    y_logit, z_mean, z_logsigma = self.encode(x)\n",
    "    z = self.reparameterize(z_mean, z_logsigma)\n",
    "    recon = self.decode(z)\n",
    "    return y_logit, z_mean, z_logsigma, recon\n",
    "\n",
    "  def predict(self, x):\n",
    "    y_logit, z_mean, z_logsigma = self.encode(x)\n",
    "    return y_logit\n",
    "\n",
    "dbvae = DB_VAE(latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISIC Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Examine Dataset ###\n",
    "with h5py.File(f'{CWD}/datasets/ISIC/ISIC.h5','r') as f:\n",
    "    # Print the keys (names) of all groups and datasets in the file\n",
    "    print(\"Keys:\", list(f.keys()))\n",
    "\n",
    "    # Iterate through each key and print more detailed information\n",
    "    for key in f.keys():\n",
    "        if isinstance(f[key], h5py.Dataset):\n",
    "            print(f\"Dataset: {key}\")\n",
    "            print(\"  Shape:\", f[key].shape)\n",
    "            print(\"  Data type:\", f[key].dtype)\n",
    "\n",
    "### Instantiate Loader Function ###\n",
    "path_to_training_data= f'{CWD}/datasets/ISIC/ISIC.h5'\n",
    "loader_ISIC = mdl.lab2.TrainingDatasetLoader(path_to_training_data)\n",
    "\n",
    "### Visualize our data ###\n",
    "number_of_training_examples = loader_ISIC.get_train_size()\n",
    "print(number_of_training_examples)\n",
    "(images, labels) = loader_ISIC.get_batch(100)\n",
    "malignant_images = images[np.where(labels==1)[0]]\n",
    "benign_images = images[np.where(labels==0)[0]]\n",
    "\n",
    "idx_malignant = 23\n",
    "idx_benign = 9\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(malignant_images[idx_malignant])\n",
    "plt.title(\"Malignant\"); plt.grid(False)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(benign_images[idx_benign])\n",
    "plt.title(\"Benign\"); plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISIC_DiDI Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Examine Dataset ###\n",
    "with h5py.File(f'{CWD}/datasets/ISIC_DiDI/ISIC_DiDI.h5','r') as f:\n",
    "    # Print the keys (names) of all groups and datasets in the file\n",
    "    print(\"Keys:\", list(f.keys()))\n",
    "\n",
    "    # Iterate through each key and print more detailed information\n",
    "    for key in f.keys():\n",
    "        if isinstance(f[key], h5py.Dataset):\n",
    "            print(f\"Dataset: {key}\")\n",
    "            print(\"  Shape:\", f[key].shape)\n",
    "            print(\"  Data type:\", f[key].dtype)\n",
    "\n",
    "### Instantiate Loader Function ###\n",
    "path_to_training_data= f'{CWD}/datasets/ISIC_DiDI/ISIC_DiDI.h5'\n",
    "loader_ISIC_DiDI = mdl.lab2.TrainingDatasetLoader(path_to_training_data)\n",
    "\n",
    "### Visualize our data ###\n",
    "number_of_training_examples = loader_ISIC_DiDI.get_train_size()\n",
    "print(number_of_training_examples)\n",
    "(images, labels) = loader_ISIC_DiDI.get_batch(100)\n",
    "malignant_images = images[np.where(labels==1)[0]]\n",
    "benign_images = images[np.where(labels==0)[0]]\n",
    "\n",
    "idx_malignant = 23\n",
    "idx_benign = 9\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(malignant_images[idx_malignant])\n",
    "plt.title(\"Malignant\"); plt.grid(False)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(benign_images[idx_benign])\n",
    "plt.title(\"Benign\"); plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISIC_ArGI Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Examine Dataset ###\n",
    "with h5py.File(f'{CWD}/datasets/ISIC_ArGI/ISIC_ArGI.h5','r') as f:\n",
    "    # Print the keys (names) of all groups and datasets in the file\n",
    "    print(\"Keys:\", list(f.keys()))\n",
    "\n",
    "    # Iterate through each key and print more detailed information\n",
    "    for key in f.keys():\n",
    "        if isinstance(f[key], h5py.Dataset):\n",
    "            print(f\"Dataset: {key}\")\n",
    "            print(\"  Shape:\", f[key].shape)\n",
    "            print(\"  Data type:\", f[key].dtype)\n",
    "\n",
    "### Instantiate Loader Function ###\n",
    "path_to_training_data= f'{CWD}/datasets/ISIC_ArGI/ISIC_ArGI.h5'\n",
    "loader_ISIC_ArGI = mdl.lab2.TrainingDatasetLoader(path_to_training_data)\n",
    "\n",
    "### Visualize our data ###\n",
    "number_of_training_examples = loader_ISIC_DiDI.get_train_size()\n",
    "print(number_of_training_examples)\n",
    "(images, labels) = loader_ISIC_ArGI.get_batch(100)\n",
    "malignant_images = images[np.where(labels==1)[0]]\n",
    "benign_images = images[np.where(labels==0)[0]]\n",
    "\n",
    "idx_malignant = 23\n",
    "idx_benign = 9\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(malignant_images[idx_malignant])\n",
    "plt.title(\"Malignant\"); plt.grid(False)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(benign_images[idx_benign])\n",
    "plt.title(\"Benign\"); plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - Standard ResNet50V2, trained on ISIC alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = make_standard_ResNet50_V2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the standard CNN ###\n",
    "\n",
    "# Training hyperparameters\n",
    "params = dict(\n",
    "  batch_size = 32,\n",
    "  num_epochs = 50,  # keep small to run faster\n",
    "  learning_rate = 5e-4,\n",
    ")\n",
    "\n",
    "experiment = create_experiment(\"Model_1\", params)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(params[\"learning_rate\"]) # define our optimizer\n",
    "loss_history = mdl.util.LossHistory(smoothing_factor=0.99) # to record loss evolution\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, scale='semilogy')\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "@tf.function\n",
    "def standard_train_step(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # feed the images into the model\n",
    "    logits = model_1(x)\n",
    "    # Compute the loss\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "\n",
    "  # Backpropagation\n",
    "  grads = tape.gradient(loss, model_1.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model_1.trainable_variables))\n",
    "  return loss\n",
    "\n",
    "# The training loop!\n",
    "step = 0\n",
    "for epoch in range(params[\"num_epochs\"]):\n",
    "  for idx in tqdm(range(loader_ISIC.get_train_size()//params[\"batch_size\"])):\n",
    "    # Grab a batch of training data and propagate through the network\n",
    "    x, y = loader_ISIC.get_batch(params[\"batch_size\"])\n",
    "\n",
    "    loss = standard_train_step(x, y)\n",
    "\n",
    "    # Record the loss and plot the evolution of the loss as a function of training\n",
    "    loss_history.append(loss.numpy().mean())\n",
    "    plotter.plot(loss_history.get())\n",
    "\n",
    "    experiment.log_metric(\"loss\", loss.numpy().mean(), step=step)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain Loss Values Over Epoch ###\n",
    "steps = len(loss_history.get())\n",
    "print(steps)\n",
    "\n",
    "epochs = np.uint8(params['num_epochs'])\n",
    "batches_per_epoch = np.uint8(steps/epochs)\n",
    "\n",
    "loss_hist = np.zeros((steps,1))\n",
    "for i in range(steps):\n",
    "    loss_hist[i] = loss_history.get()[i]\n",
    "loss_hist = loss_hist.reshape(epochs, batches_per_epoch)\n",
    "\n",
    "row_means = np.zeros((epochs))\n",
    "for i in range(epochs):\n",
    "    row_means[i] = loss_hist.sum(axis=1)[i]\n",
    "\n",
    "row_means = row_means/batches_per_epoch\n",
    "for mean in row_means:\n",
    "    print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Loss\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = \"Times New Roman\"\n",
    "epochs_range = np.arange(1,51)\n",
    "plt.figure(figsize=(8,7))\n",
    "size_axis_titles = 16\n",
    "size_title = 18\n",
    "size_legend = 14\n",
    "plt.xlabel(\"Epoch\", fontsize=size_axis_titles)\n",
    "plt.ylabel(\"Loss\", fontsize = size_axis_titles)\n",
    "plt.axis([1, 50, 0, 2.5])\n",
    "plt.plot(epochs_range,row_means,label='Training')\n",
    "plt.legend(loc='upper right', fontsize=size_legend)\n",
    "plt.title(f'Training Loss for Model 1', fontsize=size_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation of standard CNN on ISIC ###\n",
    "n=30\n",
    "accuracies = np.zeros((n,1))\n",
    "for i in range(n):\n",
    "    (batch_x, batch_y) = loader_ISIC.get_batch(256) # Matches batch size\n",
    "    y_pred_standard = tf.round(tf.nn.sigmoid(model_1.predict(batch_x)))\n",
    "    acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
    "    accuracies[i] = acc_standard.numpy()\n",
    "\n",
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation of standard CNN on ISIC_DiDI ###\n",
    "n=30\n",
    "accuracies = np.zeros((n,1))\n",
    "\n",
    "for i in range(n):\n",
    "    (batch_x, batch_y) = loader_ISIC_DiDI.get_batch(256) # Matches batch size\n",
    "    y_pred_standard = tf.round(tf.nn.sigmoid(model_1.predict(batch_x)))\n",
    "    acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
    "    accuracies[i] = acc_standard.numpy()\n",
    "\n",
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clear Previous Session ###\n",
    "tf.keras.backend.clear_session()\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - Standard ResNet50V2, trained on ISIC_DiDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = make_standard_ResNet50_V2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the standard CNN ###\n",
    "\n",
    "# Training hyperparameters\n",
    "params = dict(\n",
    "  batch_size = 32,\n",
    "  num_epochs = 50,  # keep small to run faster\n",
    "  learning_rate = 5e-4,\n",
    ")\n",
    "\n",
    "experiment = create_experiment(\"Model_2\", params)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(params[\"learning_rate\"]) # define our optimizer\n",
    "loss_history = mdl.util.LossHistory(smoothing_factor=0.99) # to record loss evolution\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, scale='semilogy')\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "@tf.function\n",
    "def standard_train_step(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # feed the images into the model\n",
    "    logits = model_2(x)\n",
    "    # Compute the loss\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "\n",
    "  # Backpropagation\n",
    "  grads = tape.gradient(loss, model_2.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model_2.trainable_variables))\n",
    "  return loss\n",
    "\n",
    "# The training loop!\n",
    "step = 0\n",
    "for epoch in range(params[\"num_epochs\"]):\n",
    "  for idx in tqdm(range(loader_ISIC_DiDI.get_train_size()//params[\"batch_size\"])):\n",
    "    # Grab a batch of training data and propagate through the network\n",
    "    x, y = loader_ISIC_DiDI.get_batch(params[\"batch_size\"])\n",
    "\n",
    "    loss = standard_train_step(x, y)\n",
    "\n",
    "    # Record the loss and plot the evolution of the loss as a function of training\n",
    "    loss_history.append(loss.numpy().mean())\n",
    "    plotter.plot(loss_history.get())\n",
    "\n",
    "    experiment.log_metric(\"loss\", loss.numpy().mean(), step=step)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain Loss Values Over Epoch ###\n",
    "steps = len(loss_history.get())\n",
    "print(steps)\n",
    "\n",
    "epochs = np.uint8(params['num_epochs'])\n",
    "batches_per_epoch = np.uint8(steps/epochs)\n",
    "\n",
    "loss_hist = np.zeros((steps,1))\n",
    "for i in range(steps):\n",
    "    loss_hist[i] = loss_history.get()[i]\n",
    "loss_hist = loss_hist.reshape(epochs, batches_per_epoch)\n",
    "\n",
    "row_means = np.zeros((epochs))\n",
    "for i in range(epochs):\n",
    "    row_means[i] = loss_hist.sum(axis=1)[i]\n",
    "\n",
    "row_means = row_means/batches_per_epoch\n",
    "for mean in row_means:\n",
    "    print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph Loss ###\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = \"Times New Roman\"\n",
    "epochs_range = np.arange(1,51)\n",
    "plt.figure(figsize=(8,7))\n",
    "size_axis_titles = 16\n",
    "size_title = 18\n",
    "size_legend = 14\n",
    "plt.xlabel(\"Epoch\", fontsize=size_axis_titles)\n",
    "plt.ylabel(\"Loss\", fontsize = size_axis_titles)\n",
    "plt.axis([1, 50, 0, 2.5])\n",
    "plt.plot(epochs_range,row_means,label='Training')\n",
    "plt.legend(loc='upper right', fontsize=size_legend)\n",
    "plt.title(f'Training Loss for Model 2', fontsize=size_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation of standard CNN on ISIC ###\n",
    "n=30\n",
    "accuracies = np.zeros((n,1))\n",
    "\n",
    "for i in range(n):\n",
    "    (batch_x, batch_y) = loader_ISIC.get_batch(256) # Matches batch size\n",
    "    y_pred_standard = tf.round(tf.nn.sigmoid(model_2.predict(batch_x)))\n",
    "    acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
    "    accuracies[i] = acc_standard.numpy()\n",
    "\n",
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation of standard CNN on ISIC_DiDI ###\n",
    "n=30\n",
    "accuracies = np.zeros((n,1))\n",
    "\n",
    "for i in range(n):\n",
    "    (batch_x, batch_y) = loader_ISIC_DiDI.get_batch(256) # Matches batch size\n",
    "    y_pred_standard = tf.round(tf.nn.sigmoid(model_2.predict(batch_x)))\n",
    "    acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
    "    accuracies[i] = acc_standard.numpy()\n",
    "\n",
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clear Previous Session ###\n",
    "tf.keras.backend.clear_session()\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = make_standard_ResNet50_V2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the standard CNN ###\n",
    "\n",
    "# Training hyperparameters\n",
    "params = dict(\n",
    "  batch_size = 32,\n",
    "  num_epochs = 50,  # keep small to run faster\n",
    "  learning_rate = 5e-4,\n",
    ")\n",
    "\n",
    "experiment = create_experiment(\"Model_3\", params)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(params[\"learning_rate\"]) # define our optimizer\n",
    "loss_history = mdl.util.LossHistory(smoothing_factor=0.99) # to record loss evolution\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, scale='semilogy')\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "@tf.function\n",
    "def standard_train_step(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # feed the images into the model\n",
    "    logits = model_3(x)\n",
    "    # Compute the loss\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "\n",
    "  # Backpropagation\n",
    "  grads = tape.gradient(loss, model_3.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model_3.trainable_variables))\n",
    "  return loss\n",
    "\n",
    "# The training loop!\n",
    "step = 0\n",
    "for epoch in range(params[\"num_epochs\"]):\n",
    "  for idx in tqdm(range(loader_ISIC_ArGI.get_train_size()//params[\"batch_size\"])):\n",
    "    # Grab a batch of training data and propagate through the network\n",
    "    x, y = loader_ISIC_ArGI.get_batch(params[\"batch_size\"])\n",
    "\n",
    "    loss = standard_train_step(x, y)\n",
    "\n",
    "    # Record the loss and plot the evolution of the loss as a function of training\n",
    "    loss_history.append(loss.numpy().mean())\n",
    "    plotter.plot(loss_history.get())\n",
    "\n",
    "    experiment.log_metric(\"loss\", loss.numpy().mean(), step=step)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain Loss Values Over Epoch ###\n",
    "steps = len(loss_history.get())\n",
    "print(steps)\n",
    "\n",
    "epochs = np.uint8(params['num_epochs'])\n",
    "batches_per_epoch = np.uint8(steps/epochs)\n",
    "\n",
    "loss_hist = np.zeros((steps,1))\n",
    "for i in range(steps):\n",
    "    loss_hist[i] = loss_history.get()[i]\n",
    "loss_hist = loss_hist.reshape(epochs, batches_per_epoch)\n",
    "\n",
    "row_means = np.zeros((epochs))\n",
    "for i in range(epochs):\n",
    "    row_means[i] = loss_hist.sum(axis=1)[i]\n",
    "\n",
    "row_means = row_means/batches_per_epoch\n",
    "for mean in row_means:\n",
    "    print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph Loss ###\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = \"Times New Roman\"\n",
    "epochs_range = np.arange(1,51)\n",
    "plt.figure(figsize=(8,7))\n",
    "size_axis_titles = 16\n",
    "size_title = 18\n",
    "size_legend = 14\n",
    "plt.xlabel(\"Epoch\", fontsize=size_axis_titles)\n",
    "plt.ylabel(\"Loss\", fontsize = size_axis_titles)\n",
    "plt.axis([1, 50, 0, 2.5])\n",
    "plt.plot(epochs_range,row_means,label='Training')\n",
    "plt.legend(loc='upper right', fontsize=size_legend)\n",
    "plt.title(f'Training Loss for Model 3', fontsize=size_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation of standard CNN on ISIC ###\n",
    "n=30\n",
    "accuracies = np.zeros((n,1))\n",
    "\n",
    "for i in range(n):\n",
    "    (batch_x, batch_y) = loader_ISIC.get_batch(256) # Matches batch size\n",
    "    y_pred_standard = tf.round(tf.nn.sigmoid(model_3.predict(batch_x)))\n",
    "    acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
    "    accuracies[i] = acc_standard.numpy()\n",
    "\n",
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation of standard CNN on ISIC_DiDI ###\n",
    "n=30\n",
    "accuracies = np.zeros((n,1))\n",
    "\n",
    "for i in range(n):\n",
    "    (batch_x, batch_y) = loader_ISIC_DiDI.get_batch(256) # Matches batch size\n",
    "    y_pred_standard = tf.round(tf.nn.sigmoid(model_3.predict(batch_x)))\n",
    "    acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
    "    accuracies[i] = acc_standard.numpy()\n",
    "\n",
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models 4, 5, and 6\n",
    "Between training different models, make sure to rerun the initialization code below to reset the weights of the dbvae.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbvae = DB_VAE(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DB_VAE Training Helper Functions ###\n",
    "\n",
    "# Function to return the means for an input image batch\n",
    "def get_latent_mu(images, dbvae, batch_size=1024):\n",
    "    N = images.shape[0]\n",
    "    mu = np.zeros((N, latent_dim))\n",
    "    for start_ind in range(0, N, batch_size):\n",
    "        end_ind = min(start_ind+batch_size, N+1)\n",
    "        batch = (images[start_ind:end_ind]).astype(np.float32)/255.\n",
    "        _, batch_mu, _ = dbvae.encode(batch)\n",
    "        mu[start_ind:end_ind] = batch_mu\n",
    "    return mu\n",
    "\n",
    "def get_training_sample_probabilities(images, dbvae, bins=10, smoothing_fac=0.001):\n",
    "    print(\"Recomputing the sampling probabilities\")\n",
    "    mu = get_latent_mu(images, dbvae)\n",
    "    training_sample_p = np.zeros(mu.shape[0])\n",
    "    for i in range(latent_dim):\n",
    "        latent_distribution = mu[:,i]\n",
    "        hist_density, bin_edges =  np.histogram(latent_distribution, density=True, bins=bins)\n",
    "        bin_edges[0] = -float('inf')\n",
    "        bin_edges[-1] = float('inf')\n",
    "        bin_idx = np.digitize(latent_distribution, bin_edges)\n",
    "        hist_smoothed_density = hist_density + smoothing_fac\n",
    "        hist_smoothed_density = hist_smoothed_density / np.sum(hist_smoothed_density)\n",
    "        p = 1.0/(hist_smoothed_density[bin_idx-1])\n",
    "        p = p / np.sum(p)\n",
    "        training_sample_p = np.maximum(p, training_sample_p)\n",
    "    training_sample_p /= np.sum(training_sample_p)\n",
    "\n",
    "    return training_sample_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the DB-VAE on ISIC ###\n",
    "import IPython\n",
    "# Hyperparameters\n",
    "params = dict(\n",
    "  batch_size = 32,\n",
    "  learning_rate = 5e-4,\n",
    "  latent_dim = 100,\n",
    "  num_epochs = 50, #DB-VAE needs slightly more epochs to train\n",
    ")\n",
    "\n",
    "experiment = create_experiment(\"Model_4\", params)\n",
    "\n",
    "# instantiate a new DB-VAE model and optimizer\n",
    "dbvae = DB_VAE(params[\"latent_dim\"])\n",
    "optimizer = tf.keras.optimizers.Adam(params[\"learning_rate\"])\n",
    "\n",
    "@tf.function\n",
    "def debiasing_train_step(x, y):\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    y_logit, z_mean, z_logsigma, x_recon = dbvae(x)\n",
    "    loss, class_loss = debiasing_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma)\n",
    "  grads = tape.gradient(loss, dbvae.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, dbvae.trainable_variables))\n",
    "  return loss\n",
    "\n",
    "all_imgs = loader_ISIC.get_all_train_faces()\n",
    "\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "loss_history_2 = mdl.util.LossHistory(smoothing_factor=0.99) # to record loss evolution\n",
    "# The training loop -- outer loop iterates over the number of epochs\n",
    "step = 0\n",
    "for i in range(params[\"num_epochs\"]):\n",
    "\n",
    "  IPython.display.clear_output(wait=True)\n",
    "  print(\"Starting epoch {}/{}\".format(i+1, params[\"num_epochs\"]))\n",
    "  p_lesions = get_training_sample_probabilities(all_imgs, dbvae)\n",
    "\n",
    "  for j in tqdm(range(loader_ISIC.get_train_size() // params[\"batch_size\"])):\n",
    "    # load a batch of data\n",
    "    (x, y) = loader_ISIC.get_batch(params[\"batch_size\"], p_pos=p_lesions)\n",
    "\n",
    "    # loss optimization\n",
    "    loss = debiasing_train_step(x, y)\n",
    "    experiment.log_metric(\"loss\", loss.numpy().mean(), step=step, epoch=i+1)\n",
    "    loss_history_2.append(loss.numpy().mean())\n",
    "    # plot the progress every 200 steps\n",
    "    if j % 500 == 0:\n",
    "      mdl.util.plot_sample(x, y, dbvae)\n",
    "\n",
    "    step += 1\n",
    "\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the DB-VAE on ISIC + DiDI ###\n",
    "import IPython\n",
    "# Hyperparameters\n",
    "params = dict(\n",
    "  batch_size = 32,\n",
    "  learning_rate = 5e-4,\n",
    "  latent_dim = 100,\n",
    "  num_epochs = 50, #DB-VAE needs slightly more epochs to train\n",
    ")\n",
    "\n",
    "experiment = create_experiment(\"Model_4\", params)\n",
    "\n",
    "# instantiate a new DB-VAE model and optimizer\n",
    "dbvae = DB_VAE(params[\"latent_dim\"])\n",
    "optimizer = tf.keras.optimizers.Adam(params[\"learning_rate\"])\n",
    "\n",
    "@tf.function\n",
    "def debiasing_train_step(x, y):\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    y_logit, z_mean, z_logsigma, x_recon = dbvae(x)\n",
    "    loss, class_loss = debiasing_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma)\n",
    "  grads = tape.gradient(loss, dbvae.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, dbvae.trainable_variables))\n",
    "  return loss\n",
    "\n",
    "all_imgs = loader_ISIC_DiDI.get_all_train_faces()\n",
    "\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "loss_history_2 = mdl.util.LossHistory(smoothing_factor=0.99) # to record loss evolution\n",
    "# The training loop -- outer loop iterates over the number of epochs\n",
    "step = 0\n",
    "for i in range(params[\"num_epochs\"]):\n",
    "\n",
    "  IPython.display.clear_output(wait=True)\n",
    "  print(\"Starting epoch {}/{}\".format(i+1, params[\"num_epochs\"]))\n",
    "  p_lesions = get_training_sample_probabilities(all_imgs, dbvae)\n",
    "\n",
    "  for j in tqdm(range(loader_ISIC_DiDI.get_train_size() // params[\"batch_size\"])):\n",
    "    # load a batch of data\n",
    "    (x, y) = loader_ISIC_DiDI.get_batch(params[\"batch_size\"], p_pos=p_lesions)\n",
    "\n",
    "    # loss optimization\n",
    "    loss = debiasing_train_step(x, y)\n",
    "    experiment.log_metric(\"loss\", loss.numpy().mean(), step=step, epoch=i+1)\n",
    "    loss_history_2.append(loss.numpy().mean())\n",
    "    # plot the progress every 200 steps\n",
    "    if j % 500 == 0:\n",
    "      mdl.util.plot_sample(x, y, dbvae)\n",
    "\n",
    "    step += 1\n",
    "\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the DB-VAE on ISIC + ArGI ###\n",
    "import IPython\n",
    "# Hyperparameters\n",
    "params = dict(\n",
    "  batch_size = 32,\n",
    "  learning_rate = 5e-4,\n",
    "  latent_dim = 100,\n",
    "  num_epochs = 50, #DB-VAE needs slightly more epochs to train\n",
    ")\n",
    "\n",
    "experiment = create_experiment(\"Model_4\", params)\n",
    "\n",
    "# instantiate a new DB-VAE model and optimizer\n",
    "dbvae = DB_VAE(params[\"latent_dim\"])\n",
    "optimizer = tf.keras.optimizers.Adam(params[\"learning_rate\"])\n",
    "\n",
    "@tf.function\n",
    "def debiasing_train_step(x, y):\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    y_logit, z_mean, z_logsigma, x_recon = dbvae(x)\n",
    "    loss, class_loss = debiasing_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma)\n",
    "  grads = tape.gradient(loss, dbvae.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, dbvae.trainable_variables))\n",
    "  return loss\n",
    "\n",
    "all_imgs = loader_ISIC_ArGI.get_all_train_faces()\n",
    "\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "loss_history_2 = mdl.util.LossHistory(smoothing_factor=0.99) # to record loss evolution\n",
    "# The training loop -- outer loop iterates over the number of epochs\n",
    "step = 0\n",
    "for i in range(params[\"num_epochs\"]):\n",
    "\n",
    "  IPython.display.clear_output(wait=True)\n",
    "  print(\"Starting epoch {}/{}\".format(i+1, params[\"num_epochs\"]))\n",
    "  p_lesions = get_training_sample_probabilities(all_imgs, dbvae)\n",
    "\n",
    "  for j in tqdm(range(loader_ISIC_ArGI.get_train_size() // params[\"batch_size\"])):\n",
    "    # load a batch of data\n",
    "    (x, y) = loader_ISIC_ArGI.get_batch(params[\"batch_size\"], p_pos=p_lesions)\n",
    "\n",
    "    # loss optimization\n",
    "    loss = debiasing_train_step(x, y)\n",
    "    experiment.log_metric(\"loss\", loss.numpy().mean(), step=step, epoch=i+1)\n",
    "    loss_history_2.append(loss.numpy().mean())\n",
    "    # plot the progress every 200 steps\n",
    "    if j % 500 == 0:\n",
    "      mdl.util.plot_sample(x, y, dbvae)\n",
    "\n",
    "    step += 1\n",
    "\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain Loss Values Over Epoch ###\n",
    "steps = len(loss_history_2.get())\n",
    "print(steps)\n",
    "\n",
    "epochs = np.uint8(params['num_epochs'])\n",
    "batches_per_epoch = np.uint8(steps/epochs)\n",
    "\n",
    "loss_hist = np.zeros((steps,1))\n",
    "for i in range(steps):\n",
    "    loss_hist[i] = loss_history_2.get()[i]\n",
    "loss_hist = loss_hist.reshape(epochs, batches_per_epoch)\n",
    "\n",
    "row_means = np.zeros((epochs))\n",
    "for i in range(epochs):\n",
    "    row_means[i] = loss_hist.sum(axis=1)[i]\n",
    "\n",
    "row_means = row_means/batches_per_epoch\n",
    "for mean in row_means:\n",
    "    print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation of standard CNN on ISIC ###\n",
    "n=30\n",
    "accuracies = np.zeros((n,1))\n",
    "\n",
    "for i in range(n):\n",
    "    (batch_x, batch_y) = loader_ISIC.get_batch(256) # Matches batch size\n",
    "    y_pred_standard = tf.round(tf.nn.sigmoid(dbvae.predict(batch_x)))\n",
    "    acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
    "    accuracies[i] = acc_standard.numpy()\n",
    "\n",
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation of standard CNN on ISIC_DiDI ###\n",
    "n=30\n",
    "accuracies = np.zeros((n,1))\n",
    "\n",
    "for i in range(n):\n",
    "    (batch_x, batch_y) = loader_ISIC_DiDI.get_batch(256) # Matches batch size\n",
    "    y_pred_standard = tf.round(tf.nn.sigmoid(dbvae.predict(batch_x)))\n",
    "    acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
    "    accuracies[i] = acc_standard.numpy()\n",
    "\n",
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
